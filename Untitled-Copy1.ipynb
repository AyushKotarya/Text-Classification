{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15291544-c446-4ffa-aebd-0fc68ebd71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\")\n",
    "\n",
    "emotions\n",
    "\n",
    "train = emotions[\"train\"]\n",
    "train # its basically a hugging dataset object normally act as python list\n",
    "\n",
    "len(train) \n",
    "\n",
    "train[0] # in this list each row is represented as a dict with two key-value pairs\n",
    "\n",
    "train.column_names\n",
    "\n",
    "train.features # text in string & label in special ClassLabel object \n",
    "\n",
    "train[0] # a dict in short\n",
    "\n",
    "train[0][\"text\"] # accessing value from dictionary\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "emotions.set_format(type=\"pandas\") # using set_format from hugging dataset object\n",
    "\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()\n",
    "\n",
    "emotions[\"train\"]\n",
    "emotions[\"train\"].features\n",
    "emotions[\"train\"].features[\"label\"]\n",
    "emotions[\"train\"].features[\"label\"].int2str(3)\n",
    "\n",
    "def label_enc(label):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(label)\n",
    "\n",
    "df[\"label_enc\"] = df[\"label\"].apply(label_enc) # making new column for decoding the integers aka labels\n",
    "df\n",
    "\n",
    "# EDA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label_enc\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = df[\"text\"]\n",
    "y_train = df[\"label\"]\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply oversampling\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Create a DataFrame with the resampled TF-IDF features\n",
    "df_resampled_tfidf = pd.DataFrame(X_resampled.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Find the nearest neighbors in the original dataset for each resampled point\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
    "nn.fit(X_train_tfidf)\n",
    "indices = nn.kneighbors(X_resampled, return_distance=False)\n",
    "\n",
    "# Create DataFrame with nearest original text and resampled labels\n",
    "df_resampled = pd.DataFrame({\n",
    "    'text': X_train.iloc[indices.flatten()].values,\n",
    "    'label': y_resampled\n",
    "})\n",
    "\n",
    "print(df_resampled['label'].value_counts())\n",
    "\n",
    "df_resampled\n",
    "\n",
    "df_resampled[\"label\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()\n",
    "\n",
    "df_resampled[\"text\"].str\n",
    "\n",
    "# df[\"text\"].split()\n",
    "# therefore applied .str earlier\n",
    "df_resampled[\"text\"].str.split()\n",
    "\n",
    "df_resampled[\"text\"].str.split().apply(len)\n",
    "\n",
    "df_resampled[\"Words Per Tweet\"] = df_resampled[\"text\"].str.split().apply(len)\n",
    "\n",
    "df_resampled\n",
    "\n",
    "df_resampled.boxplot(\"Words Per Tweet\", by=\"label\", grid=False,\n",
    "showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "emotions.reset_format()\n",
    "\n",
    "type(emotions) # But I have to use resampled datset isnt so less see what we can do\n",
    "\n",
    "# Creating a hugginf face dataset object for resampled data\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Assuming df_resampled is your DataFrame with 'text' and 'label' columns\n",
    "# Convert DataFrame to dictionary\n",
    "data_dict = {\n",
    "    'text': df_resampled['text'].tolist(),\n",
    "    'label': df_resampled['label'].tolist()\n",
    "}\n",
    "\n",
    "# Create the Dataset object\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Create a DatasetDict with a 'train' split\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset\n",
    "})\n",
    "\n",
    "# Print the structure\n",
    "print(dataset_dict)\n",
    "\n",
    "dataset_dict.reset_format()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "text = \"Tokenizing text is the core task of NLP\"\n",
    "encoded_text = tokenizer(text)\n",
    "encoded_text\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "\n",
    "tokens # non common words such as NLP and tokenizing is halded differently i.e. part by part\n",
    "\n",
    "print(tokenizer.convert_tokens_to_string(tokens)) # text lowered automatically \n",
    "\n",
    "tokenizer.vocab_size  # cause its a pretrained tokenizer bro so thats why this massive size beforehand \n",
    "\n",
    "tokenizer.model_max_length # trucation limit token numbers but we are good to go we know it \n",
    "\n",
    "# Lets deine a processing function which will used to tokenize whole corpus heheheh\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=True, truncation=True)\n",
    "\n",
    "emotions_encoded = dataset_dict.map(tokenize, batched=True, batch_size=None) # here we are doing with unsampled dataset \n",
    "\n",
    "print(emotions_encoded[\"train\"].column_names)\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModel\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "# Automodel se pretrained model use kr rhe hai to create embeedings and feed through the encode stack to return hidden states (which we are gonna use \n",
    "# as features for classification task)\n",
    "\n",
    "# extracting final hidden state \n",
    "# to do that we need to define a new function to preprocess on the whole corpus \n",
    "# lets first see and do that on a normal text then apply on the wholw corpus \n",
    "\n",
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n",
    "\n",
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "outputs.last_hidden_state.size()\n",
    "\n",
    "outputs.last_hidden_state[:,0].size()\n",
    "\n",
    "def extract_hidden_states(batch):\n",
    "# Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "        if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "emotions_encoded.set_format(\"torch\",\n",
    "columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)\n",
    "\n",
    "# see now we need to trwin our classifier and we need valid dataset for same but in our data_resampled we dont have and we can diretly use the dagtaset \n",
    "# imbalnced one for calidation task so its good to go legts make embeddings for validation dataste as well \n",
    "\n",
    "# encoding for validation dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],padding=True, truncation=True)\n",
    "\n",
    "emotions_encoded_valid = emotions.map(tokenize, batched=True, batch_size=None) \n",
    "\n",
    "# embeddings and hidden states for validdation dataset\n",
    "def extract_hidden_states(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "        if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "emotions_encoded_valid.set_format(\"torch\",\n",
    "columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "emotions_hidden_valid = emotions_encoded_valid.map(extract_hidden_states, batched=True)\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden_valid[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden_valid[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape\n",
    "\n",
    "# Saved Embeddings \n",
    "\n",
    "# Save the dataset\n",
    "emotions_hidden.save_to_disk(\"./emotions_hidden\")\n",
    "\n",
    "# Later, to load the dataset:\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "# emotions_hidden = load_from_disk(\"path/to/save/emotions_hidden\")\n",
    "\n",
    "# Save the dataset\n",
    "emotions_hidden_valid.save_to_disk(\"./emotions_hidden_valid\")\n",
    "\n",
    "# Later, to load the dataset:\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "# emotions_hidden_valid = load_from_disk(\"path/to/save/emotions_hidden\")\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "        gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# We increase `max_iter` to guarantee convergence\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_clf.score(X_valid, y_valid)\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_valid, y_valid)\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)\n",
    "\n",
    "# Second Approach for more better classification\n",
    "# Aka fine tuning approach\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "num_labels = 6\n",
    "model = (AutoModelForSequenceClassification\n",
    ".from_pretrained(model_ckpt, num_labels=num_labels)\n",
    ".to(device))\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                num_train_epochs=2,\n",
    "                learning_rate=2e-5,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                weight_decay=0.01,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                disable_tqdm=False,\n",
    "                logging_steps=logging_steps,\n",
    "                push_to_hub=True,\n",
    "                log_level=\"error\")\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=emotions_encoded[\"train\"],\n",
    "            eval_dataset=emotions_encoded_valid[\"validation\"],\n",
    "            tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "# Saving the trained model\n",
    "\n",
    "# Save the model\n",
    "model_path = \"./model\"\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save the training arguments\n",
    "trainer.save_state()\n",
    "\n",
    "# To load model \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# Load the model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the training arguments\n",
    "training_args = TrainingArguments.from_pretrained(model_path)\n",
    "\n",
    "# Create a new Trainer instance with the loaded model\n",
    "new_trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    tokenizer=loaded_tokenizer\n",
    ")\n",
    "\n",
    "# Now you can use new_trainer for inference or continue training\n",
    "\n",
    "# Cont.\n",
    "\n",
    "preds_output = trainer.predict(emotions_encoded_valid[\"validation\"])\n",
    "\n",
    "preds_output.metrics\n",
    "\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "# Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "        reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "    \"predicted_label\": pred_label.cpu().numpy()}\n",
    "            \n",
    "\n",
    "# Convert our dataset back to PyTorch tensors\n",
    "emotions_encoded_valid.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "emotions_encoded_valid[\"validation\"] = emotions_encoded_valid[\"validation\"].map(\n",
    "                                            forward_pass_with_label, batched=True, batch_size=16)\n",
    "\n",
    "emotions_encoded_valid.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded_valid[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_enc)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].apply(label_enc))\n",
    "\n",
    "df_test.sort_values(\"loss\", ascending=False).head(10)\n",
    "\n",
    "df_test.sort_values(\"loss\", ascending=True).head(10)\n",
    "\n",
    "# Resaving model\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "# Change `transformersbook` to your Hub username\n",
    "model_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "preds = classifier(custom_tweet, return_all_scores=True)\n",
    "\n",
    "preds_df = pd.DataFrame(preds[0])\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
